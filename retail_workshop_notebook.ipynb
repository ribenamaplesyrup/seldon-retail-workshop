{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "retail_workshop_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hmI8brYLieU-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqU0bqjtRKwu"
      },
      "source": [
        "# Customer Segmentation \n",
        "\n",
        "This notebook will focuss on the creation, deployment, \n",
        "monitoring and management of a machine learning model for performing \n",
        "customer segmentation. We will be using an e-commerce dataset detailing actual purchases made by  ∼ 4000 customers over a period of one year (from 2010/12/01 to 2011/12/09). \n",
        "\n",
        "In this notebook we will: \n",
        "\n",
        "- explore subset of dataset\n",
        "- train several models on pre-processed dataset\n",
        "- deploy trained models to Seldon \n",
        "- train an anchor tabular explainer and update Seldon deployment with explainer\n",
        "- train an outlier detector (variational autoencoder) and drift detector (k-s tests) and update deployment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCnrhNBSkOZW"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNI8xSiiBd24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14df01cc-ecc1-4451-b4a8-0579f1f4eb45"
      },
      "source": [
        "!pip install seldon-deploy-sdk\n",
        "!pip install alibi\n",
        "!pip install alibi-detect\n",
        "!pip install fsspec\n",
        "!pip install gcsfs\n",
        "!pip install dill"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seldon-deploy-sdk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/52/f6ca471d20aa42d25c52e3d1fe119985ba40ab03031ca3cbf0ea6582fa9a/seldon_deploy_sdk-0.2.1-py3-none-any.whl (613kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.23 in /usr/local/lib/python3.7/dist-packages (from seldon-deploy-sdk) (1.24.3)\n",
            "Collecting Authlib<=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ee/038fdffd329825d5e026ee6e81b945938c10232794d08dcbacfbbd23dce0/Authlib-0.15.3-py2.py3-none-any.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from seldon-deploy-sdk) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from seldon-deploy-sdk) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from seldon-deploy-sdk) (2020.12.5)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->Authlib<=0.16.0->seldon-deploy-sdk) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->Authlib<=0.16.0->seldon-deploy-sdk) (2.20)\n",
            "Installing collected packages: cryptography, Authlib, seldon-deploy-sdk\n",
            "Successfully installed Authlib-0.15.3 cryptography-3.4.7 seldon-deploy-sdk-0.2.1\n",
            "Collecting alibi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/29/d971245c07646cb5b5ed7e9203a7f013573f9e90134d1b8f159cc57ea58d/alibi-0.5.8-py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.19.5)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (3.2.2)\n",
            "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.1.5)\n",
            "Requirement already satisfied: attrs<21.0.0,>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (20.3.0)\n",
            "Requirement already satisfied: scikit-image!=0.17.1,<0.19,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.16.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.2; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from alibi) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow<2.5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.4.1)\n",
            "Requirement already satisfied: Pillow<9.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi) (7.1.2)\n",
            "Requirement already satisfied: spacy[lookups]<4.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.2.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.4.1)\n",
            "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.3.3)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.22.2.post1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi) (2018.9)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (2.5.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (2.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (2.10)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.15.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.2.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (2.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (56.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.0.5)\n",
            "Collecting spacy-lookups-data<0.2.0,>=0.0.5; extra == \"lookups\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/4a37ca7d0c21dc2287a8bb5d249f5f3211cdf3d598acf742bf5bb8c87169/spacy_lookups_data-0.1.0.tar.gz (28.0MB)\n",
            "\u001b[K     |████████████████████████████████| 28.0MB 148kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.20.2->alibi) (1.0.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (4.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (0.4.8)\n",
            "Building wheels for collected packages: spacy-lookups-data\n",
            "  Building wheel for spacy-lookups-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.1.0-py2.py3-none-any.whl size=28052145 sha256=31998e0cd9b65f5a6cdba13a97b2eefea6021016a8be9746e1a1d265ff692029\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/2b/0a/d6fb6235c56d014d224bca760d15d7cbdd820813085ffcd35d\n",
            "Successfully built spacy-lookups-data\n",
            "Installing collected packages: alibi, spacy-lookups-data\n",
            "Successfully installed alibi-0.5.8 spacy-lookups-data-0.1.0\n",
            "Collecting alibi-detect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/5f/587ff52108e4a452db41d5f993ddbf72aa569dab9a7eeb3eccb6e6a2e57b/alibi_detect-0.6.1-py3-none-any.whl (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image!=0.17.1,<0.19,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.16.2)\n",
            "Requirement already satisfied: Pillow<9.0.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (7.1.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.19.5)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (3.2.2)\n",
            "Requirement already satisfied: tensorflow<2.5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (2.4.1)\n",
            "Collecting transformers<5.0.0,>=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 15.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python<5.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (4.1.2.30)\n",
            "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.1.5)\n",
            "Requirement already satisfied: tensorflow-probability<0.13.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.12.1)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.22.2.post1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi-detect) (2.5.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi-detect) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi-detect) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.8.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.12.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi-detect) (2.4.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=2.10.0->alibi-detect) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=2.10.0->alibi-detect) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=2.10.0->alibi-detect) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=2.10.0->alibi-detect) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=2.10.0->alibi-detect) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=2.10.0->alibi-detect) (3.10.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi-detect) (2018.9)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi-detect) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi-detect) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi-detect) (0.1.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.20.2->alibi-detect) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.5.0,>=2.0.0->alibi-detect) (56.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (3.3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=2.10.0->alibi-detect) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=2.10.0->alibi-detect) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=2.10.0->alibi-detect) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=2.10.0->alibi-detect) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=2.10.0->alibi-detect) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=2.10.0->alibi-detect) (7.1.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi-detect) (3.1.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, alibi-detect\n",
            "Successfully installed alibi-detect-0.6.1 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2021.4.0\n",
            "Collecting gcsfs\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/a1/f7e7843b89d7a097969ccb3b4f53d626108a73f31c2979e773ce4431ea2d/gcsfs-2021.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.4)\n",
            "Requirement already satisfied: fsspec==2021.04.0 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2021.4.0)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.28.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 31.5MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 28.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (20.3.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (56.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Installing collected packages: multidict, yarl, async-timeout, aiohttp, gcsfs\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 gcsfs-2021.4.0 multidict-5.1.0 yarl-1.6.3\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (0.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9RurRkiZljs"
      },
      "source": [
        "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import neighbors, linear_model, svm, tree, ensemble\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pickle \n",
        "import joblib\n",
        "import dill\n",
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Dense, InputLayer\n",
        "from alibi.explainers import AnchorTabular\n",
        "from alibi_detect.datasets import fetch_kdd\n",
        "from alibi_detect.models.tensorflow.losses import elbo\n",
        "from alibi_detect.od import OutlierVAE\n",
        "from alibi_detect.cd import TabularDrift\n",
        "from alibi_detect.utils.data import create_outlier_batch\n",
        "from alibi_detect.utils.fetching import fetch_detector\n",
        "from alibi_detect.utils.saving import save_detector, load_detector\n",
        "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_tabular, plot_roc\n",
        "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, OutlierDetectorApi, DriftDetectorApi\n",
        "from seldon_deploy_sdk.auth import OIDCAuthenticator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr6yO693qzfH"
      },
      "source": [
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYt8NSjqxTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fa5225-d522-4d88-d536-09cba0c4758d"
      },
      "source": [
        "!gsutil -m cp -r gs://tom-seldon-examples/datareply-workshop/data ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tom-seldon-examples/datareply-workshop/data/X_test.npy...\n",
            "/ [0/6 files][    0.0 B/ 43.6 MiB]   0% Done                                    \rCopying gs://tom-seldon-examples/datareply-workshop/data/X_train.npy...\n",
            "/ [0/6 files][    0.0 B/ 43.6 MiB]   0% Done                                    \rCopying gs://tom-seldon-examples/datareply-workshop/data/Y_train.npy...\n",
            "/ [0/6 files][    0.0 B/ 43.6 MiB]   0% Done                                    \rCopying gs://tom-seldon-examples/datareply-workshop/data/data.csv...\n",
            "Copying gs://tom-seldon-examples/datareply-workshop/data/Y_test.npy...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX6yZr4FRc3t"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "Typically e-commerce datasets are proprietary and consequently hard to find among publicly available data. However, The UCI Machine Learning Repository has curated a retail dataset containing actual transactions from 2010 and 2011. The dataset is maintained on their site, where it can be found by the title \"Online Retail\". The following description is provided via Kaggle: \n",
        "\n",
        "\"This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\"\n",
        "\n",
        "We will first explore the dataset and then train models on a pre-processed version of the dataset that was transformed through following the fantastic work of Fabien Daniel whose methods are detailed in this notebook: https://www.kaggle.com/fabiendaniel/customer-segmentation\n",
        "\n",
        "Lets dive in and explore a subset of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOYDKkTIcej4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba125e4-54c6-4c75-a95e-7a8b68550752"
      },
      "source": [
        "df_initial = pd.read_csv('data/data.csv', encoding=\"ISO-8859-1\", nrows=3000,\n",
        "                         dtype={'CustomerID': str,'InvoiceID': str})\n",
        "\n",
        "print('Dataframe dimensions:', df_initial.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataframe dimensions: (3000, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI8vVdPwSY0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "a2657ec9-d3e7-4e48-9643-144729e5ec3f"
      },
      "source": [
        "df_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n",
        "\n",
        "# gives some infos on columns types and numer of null values\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "display(tab_info)\n",
        "\n",
        "# show first lines\n",
        "display(df_initial[:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>InvoiceNo</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>InvoiceDate</th>\n",
              "      <th>UnitPrice</th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>column type</th>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>int64</td>\n",
              "      <td>datetime64[ns]</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>null values (nb)</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1081</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>null values (%)</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.0333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 InvoiceNo StockCode Description  ... UnitPrice CustomerID Country\n",
              "column type         object    object      object  ...   float64     object  object\n",
              "null values (nb)         0         0          10  ...         0       1081       0\n",
              "null values (%)          0         0    0.333333  ...         0    36.0333       0\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>InvoiceNo</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>InvoiceDate</th>\n",
              "      <th>UnitPrice</th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>536365</td>\n",
              "      <td>85123A</td>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-12-01 08:26:00</td>\n",
              "      <td>2.55</td>\n",
              "      <td>17850</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>536365</td>\n",
              "      <td>71053</td>\n",
              "      <td>WHITE METAL LANTERN</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-12-01 08:26:00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>536365</td>\n",
              "      <td>84406B</td>\n",
              "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-12-01 08:26:00</td>\n",
              "      <td>2.75</td>\n",
              "      <td>17850</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>536365</td>\n",
              "      <td>84029G</td>\n",
              "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-12-01 08:26:00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>536365</td>\n",
              "      <td>84029E</td>\n",
              "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-12-01 08:26:00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  InvoiceNo StockCode  ... CustomerID         Country\n",
              "0    536365    85123A  ...      17850  United Kingdom\n",
              "1    536365     71053  ...      17850  United Kingdom\n",
              "2    536365    84406B  ...      17850  United Kingdom\n",
              "3    536365    84029G  ...      17850  United Kingdom\n",
              "4    536365    84029E  ...      17850  United Kingdom\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wt_164pO2Bu"
      },
      "source": [
        "We have:\n",
        "\n",
        "**InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "\n",
        "**StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "\n",
        "**Description**: Product (item) name. Nominal.\n",
        "\n",
        "**Quantity**: The quantities of each product (item) per transaction. Numeric.\n",
        "\n",
        "**InvoiceDate**: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "\n",
        "**UnitPrice**: Unit price. Numeric, Product price per unit in sterling.\n",
        "\n",
        "**CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "\n",
        "**Country**: Country name. Nominal, the name of the country where each customer resides. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAIpLpt8QCAu"
      },
      "source": [
        "We will be training models using a transformed version of this dataset. Rather than detailing each individual item bought, the transformed dataset details the spending behavoir of each individual customer across different categories of products. We don't have any product categories in the original dataset so we create this feature through analysing keywords used within product descriptions and applying k-means clustering to create a finite number of product clusters. We find in our analysis that 5 clusters is a suitable number to seperate products without creating clusters containing too few products (calculated using silhouette score). The following wordclouds demonstrate the splits we generate: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXd0w4jAkOZa"
      },
      "source": [
        "<img src=\"https://github.com/ribenamaplesyrup/seldon-workshops/blob/main/download.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTG8hTgEXbtr"
      },
      "source": [
        "We see that cluster no1 contains words we might associate more with gifts ('christmas', 'card', 'wrap', 'decoration') and no3 more with luxury goods ('lace', 'necklace'), although some of the other clusters are less differentiated.\n",
        "\n",
        "Based on customer spending across each of these categories of product, we can similarly use k-means to cluster customers into different segments. Performing similar analysis to how we clustered product categories, we find 11 clusters is the most suitable for segmenting customers within this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNLTtkUaDIq"
      },
      "source": [
        "columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ] # change to feature_names?\n",
        "class_names = ['0','1','2','3','4','5','6','7','8','9','10']\n",
        "X_train = np.load('data/X_train.npy')\n",
        "Y_train = np.load('data/Y_train.npy')\n",
        "X_test = np.load('data/X_test.npy')\n",
        "Y_test = np.load('data/Y_test.npy')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UDuw79SVoWE"
      },
      "source": [
        "\n",
        "The following transformed features specific to each unique customer:\n",
        "\n",
        "**mean**: mean spend across all transactions\n",
        "\n",
        "**categ_0**: percentage of total spend on products within category 0\n",
        "\n",
        "**categ_1**: percentage of total spend on products within category 1\n",
        "\n",
        "**categ_2**: percentage of total spend on products within category 2\n",
        "\n",
        "**categ_3**: percentage of total spend on products within category 3\n",
        "\n",
        "**categ_4**: percentage of total spend on products within category 4\n",
        "\n",
        "**customer_category**: integer signifying which of the 11 categories the customer aligns most with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7HQJpFekOZa"
      },
      "source": [
        "### high level summarise notebook in how we get from the original dataset to the transformed dataset with \n",
        "### product category, mean and customer category features."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u24c0v7kOZa"
      },
      "source": [
        "- Mean refers to the average amount spent within each invoice. \n",
        "- Each category represents a different category of products and the value is the percentage of total spend within that category across all invoices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pOElOiWkOZa"
      },
      "source": [
        "columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ] # change to feature_names?\n",
        "class_names = ['0','1','2','3','4','5','6','7','8','9','10']\n",
        "X_train = np.load('data/X_train.npy')\n",
        "Y_train = np.load('data/Y_train.npy')\n",
        "X_test = np.load('data/X_test.npy')\n",
        "Y_test = np.load('data/Y_test.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoczMYGIkOZb"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A35NghYPkOZb"
      },
      "source": [
        "### figure out the max and min mean - useful for outliers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei4WoQXTZtjq"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "In this section we will adjust a classifier that will classify consumers in the different client categories. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZXr3-bfcrZr"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se7kBpFlcUm1"
      },
      "source": [
        "lr = linear_model.LogisticRegression(max_iter=4000)\n",
        "lr.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4mLxfzSvvAd"
      },
      "source": [
        "accuracy_score(Y_test, lr.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8KbkSIFkOZd"
      },
      "source": [
        "### Add another trained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYjuhrLrCDpL"
      },
      "source": [
        "models = ['lr']\n",
        "\n",
        "for model in models:\n",
        "    if not os.path.exists('models/' + model):\n",
        "        os.makedirs('models/' + model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68IRCKnhddNx"
      },
      "source": [
        "filename = 'models/lr/model.joblib'\n",
        "joblib.dump(lr, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zZjwLAlAoXg"
      },
      "source": [
        "### Push model artefacts to GCP\n",
        "\n",
        "To push models to GCP you will create a unique folder with your name. Uncomment the command and replace <YOUR NAME> with your name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlU3AfYG_BtN"
      },
      "source": [
        "# Push artefact to GCP\n",
        "# !gsutil cp model/<.sav model file> gs://tom-seldon-examples/datareply-workshop/models/<YOUR NAME>/<MODEL TYPE>/model.joblib\n",
        "!gsutil cp models/lr/model.joblib gs://tom-seldon-examples/datareply-workshop/models/sgreaves/lr/model.joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlrAixGMA_Hs"
      },
      "source": [
        "### Deploy models to Seldon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8XO97eBMGF"
      },
      "source": [
        "We can now deploy our model to the dedicated Seldon Deploy cluster which we have configured for this workshop. To do so we will interact with the Seldon Deploy SDK and deploy our model using that.\n",
        "\n",
        "First, setting up the configuration and authentication required to access the cluster. Make sure to fill in the SD_IP variable to be the same as the cluster you are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOCjkhaIBtVF"
      },
      "source": [
        "SD_IP = \"139.59.203.129\"\n",
        "\n",
        "config = Configuration()\n",
        "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
        "config.oidc_client_id = \"sd-api\"\n",
        "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
        "\n",
        "def auth():\n",
        "    auth = OIDCAuthenticator(config)\n",
        "    config.access_token = auth.authenticate(\"admin@seldon.io\", \"12341234\")\n",
        "    api_client = ApiClient(config)\n",
        "    return api_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajKYVfyhkOZf"
      },
      "source": [
        "### edit the following explanation ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V37sraULkOZf"
      },
      "source": [
        "Now we have configured the IP correctly as well as setup our authentication function we can desrcibe the deployment we would like to create.\n",
        "\n",
        "You will need to fill in the DEPLOYMENT_NAME, NAMESPACE, and the MODEL_LOCATION, the rest of the deployment description has been templated for you.\n",
        "\n",
        "For the MODEL_LOCATION you do not need to specify the path all the way up to model.bst e.g. if you saved your classifier under gs://tom-seldon-examples/my-workshop/tom/model.bst your MODEL_LOCATION should be gs://tom-seldon-examples/my-workshop/tom and Seldon will automatically pick up the classifier artifact stored there.\n",
        "\n",
        "You will need to create a unique deployment name. A good example format would be <YOUR NAME> + <MODEL> so \"sgreaveslogreg\" in my case (be careful not to use any upper-case letters or other characters like \"_\". We will also need to specify our model_location which needs to be the folder we pushed our model.joblib file to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aptQf11qN2W4"
      },
      "source": [
        "DEPLOYMENT_NAME = \"sgreaveslogreg\"\n",
        "MODEL_LOCATION = \"gs://tom-seldon-examples/datareply-workshop/models/sgreaves/lr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7gHzz9JBwLZ"
      },
      "source": [
        "NAMESPACE = \"test\"\n",
        "\n",
        "PREPACKAGED_SERVER = \"SKLEARN_SERVER\"\n",
        "\n",
        "CPU_REQUESTS = \"1\"\n",
        "MEMORY_REQUESTS = \"1Gi\"\n",
        "\n",
        "CPU_LIMITS = \"1\"\n",
        "MEMORY_LIMITS = \"1Gi\"\n",
        "\n",
        "mldeployment = {\n",
        "    \"kind\": \"SeldonDeployment\",\n",
        "    \"metadata\": {\n",
        "        \"name\": DEPLOYMENT_NAME,\n",
        "        \"namespace\": NAMESPACE,\n",
        "        \"labels\": {\n",
        "            \"fluentd\": \"true\"\n",
        "        }\n",
        "    },\n",
        "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
        "    \"spec\": {\n",
        "        \"name\": DEPLOYMENT_NAME,\n",
        "        \"annotations\": {\n",
        "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
        "        },\n",
        "        \"protocol\": \"seldon\",\n",
        "        \"transport\": \"rest\",\n",
        "        \"predictors\": [\n",
        "            {\n",
        "                \"componentSpecs\": [\n",
        "                    {\n",
        "                        \"spec\": {\n",
        "                            \"containers\": [\n",
        "                                {\n",
        "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
        "                                    \"resources\": {\n",
        "                                        \"requests\": {\n",
        "                                            \"cpu\": CPU_REQUESTS,\n",
        "                                            \"memory\": MEMORY_REQUESTS\n",
        "                                        },\n",
        "                                        \"limits\": {\n",
        "                                            \"cpu\": CPU_LIMITS,\n",
        "                                            \"memory\": MEMORY_LIMITS\n",
        "                                        }\n",
        "                                    }\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                ],\n",
        "                \"name\": \"default\",\n",
        "                \"replicas\": 1,\n",
        "                \"traffic\": 100,\n",
        "                \"graph\": {\n",
        "                    \"implementation\": PREPACKAGED_SERVER,\n",
        "                    \"modelUri\": MODEL_LOCATION,\n",
        "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
        "                    \"endpoint\": {\n",
        "                        \"type\": \"REST\"\n",
        "                    },\n",
        "                    \"parameters\": [],\n",
        "                    \"children\": [],\n",
        "                    \"logger\": {\n",
        "                        \"mode\": \"all\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"status\": {}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SXGONbMbSFW"
      },
      "source": [
        "We can now invoke the `SeldonDeploymentsApi` and create a new Seldon Deployment. \n",
        "\n",
        "Time for you to get your hands dirty. You will use the Seldon Deploy SDK to create a new Seldon deployment. You can find the reference documentation [here](https://github.com/SeldonIO/seldon-deploy-sdk/blob/master/python/README.md). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeKpuObJCHA3"
      },
      "source": [
        "deployment_api = SeldonDeploymentsApi(auth())\n",
        "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYIKPa8EN2W5"
      },
      "source": [
        "Our model is now running as a fully fledged microservice. We can test it by sending a request: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOc0RMrYN2W5"
      },
      "source": [
        "# test model with this request: \n",
        "{\"data\": {\"ndarray\": [[169.44666667,19.82924814,22.28823229, 28.95207932,23.49411811,5.43632215]]}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjspFW0AFlHS"
      },
      "source": [
        "## Explainer\n",
        "\n",
        "Next, we shall train an explainer to glean deeper insights into the decisions being made by our model. \n",
        "\n",
        "We will make use of the Anchors algorithm, which has a [production grade implementation available](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html) using the Seldon Alibi Explain library. \n",
        "\n",
        "The first step will be to write a simple prediction function which the explainer can call in order to query our logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMowappmHKz1"
      },
      "source": [
        "clf = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CQR0mjC6gQ"
      },
      "source": [
        "predict_fn = lambda x: clf.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNvTwo2_N2W6"
      },
      "source": [
        "We then initialise our Anchor explainer, using the AnchorTabular flavour provided by Alibi due to our data modality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNLvR53vG7Yb"
      },
      "source": [
        "explainer = AnchorTabular(predict_fn, columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QC6KMTfN2W7"
      },
      "source": [
        "# Dataset is imbalanced - is this a problem?\n",
        "# unique, counts = np.unique(Y_test, return_counts=True)\n",
        "# print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQc3nvlIHofS"
      },
      "source": [
        "explainer.fit(X_train, disc_perc=(25, 50, 75))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmnueruBN2W7"
      },
      "source": [
        "We can now test our explainer by generating a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AgHZXu3Hr52"
      },
      "source": [
        "idx = 0\n",
        "print('Prediction: ', class_names[explainer.predictor(X_test[idx].reshape(1, -1))[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o96B8o-N2W8"
      },
      "source": [
        "## explain precision and coverage "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FD98jlOI15c"
      },
      "source": [
        "explanation = explainer.explain(X_test[idx], threshold=0.9)\n",
        "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
        "print('Precision: %.2f' % explanation.precision)\n",
        "print('Coverage: %.2f' % explanation.coverage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFcxl-_hN2W8"
      },
      "source": [
        "## explain what dill is"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6LUjpMSI-BP"
      },
      "source": [
        "with open(\"models/lr/explainer.dill\", \"wb\") as model_f:\n",
        "        dill.dump(explainer, model_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JVMBQg2kOZi"
      },
      "source": [
        "### explainers are quite large files, instead of pushing to GCP we'll skip that step and deploy a trained \n",
        "### explainer already in storage "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG6kWCMDOHsR"
      },
      "source": [
        "## Deployment\n",
        "\n",
        "We can now deploy our explainer alongside our model. First we define the explainer configuration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaYB2x55kOZj"
      },
      "source": [
        "### train an explainer on the new dataset in a Python 3.6.1 environment then push to GCP and test deployment."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTq-3i5XMZDh"
      },
      "source": [
        "EXPLAINER_TYPE = \"AnchorTabular\"\n",
        "EXPLAINER_URI = \"gs://tom-seldon-examples/datareply-workshop/pretrained/lr\"\n",
        "\n",
        "explainer_spec = {\n",
        "                    \"type\": EXPLAINER_TYPE,\n",
        "                    \"modelUri\": EXPLAINER_URI,\n",
        "                    \"containerSpec\": {\n",
        "                        \"name\": \"\",\n",
        "                        \"resources\": {}\n",
        "                    }\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr7cIsDPOZ0T"
      },
      "source": [
        "mldeployment['spec']['predictors'][0]['explainer'] = explainer_spec\n",
        "mldeployment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEE1Ng-IOeR_"
      },
      "source": [
        "deployment_api = SeldonDeploymentsApi(auth())\n",
        "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmI8brYLieU-"
      },
      "source": [
        "## Outlier Detection \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTGrmaq6kOZj"
      },
      "source": [
        "### Provide explanations on all decisions made here. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBPpQneC-cqr"
      },
      "source": [
        "## Method\n",
        "\n",
        "The Variational Auto-Encoder ([VAE](https://arxiv.org/abs/1312.6114)) outlier detector is first trained on a batch of unlabeled, but normal (*inlier*) data. Unsupervised training is desireable since labeled data is often scarce. The VAE detector tries to reconstruct the input it receives. If the input data cannot be reconstructed well, the reconstruction error is high and the data can be flagged as an outlier. The reconstruction error is either measured as the mean squared error (MSE) between the input and the reconstructed instance or as the probability that both the input and the reconstructed instance are generated by the same process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qWd7Y0H-8H-"
      },
      "source": [
        "normal_batch = create_outlier_batch(X_train, Y_train, n_samples=2886, perc_outlier=0)\n",
        "X_train, y_train = normal_batch.data.astype('float'), normal_batch.target\n",
        "print(X_train.shape, y_train.shape)\n",
        "print('{}% outliers'.format(100 * y_train.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bos3TdDbDl1A"
      },
      "source": [
        "mean, stdev = X_train.mean(axis=0), X_train.std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBtHhDH8DnFo"
      },
      "source": [
        "X_train = (X_train - mean) / stdev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncuCKhNiDto3"
      },
      "source": [
        "## Load or define outlier detector\n",
        "\n",
        "The pretrained outlier and adversarial detectors used in the example notebooks can be found [here](https://console.cloud.google.com/storage/browser/seldon-models/alibi-detect). You can use the built-in ```fetch_detector``` function which saves the pre-trained models in a local directory ```filepath``` and loads the detector. Alternatively, you can train a detector from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwx_xqTwDp7O"
      },
      "source": [
        "load_outlier_detector = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSvBvODuDxtb"
      },
      "source": [
        "filepath = '/content/models/OutlierVAE'\n",
        "if not os.path.exists(filepath):\n",
        "  os.mkdir(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZewHIB-EBkz"
      },
      "source": [
        "# define model, initialize, train and save outlier detector\n",
        "    \n",
        "n_features = X_train.shape[1]\n",
        "latent_dim = 2\n",
        "    \n",
        "encoder_net = tf.keras.Sequential(\n",
        "    [\n",
        "     InputLayer(input_shape=(n_features,)),\n",
        "     Dense(20, activation=tf.nn.relu),\n",
        "     Dense(15, activation=tf.nn.relu),\n",
        "     Dense(7, activation=tf.nn.relu)\n",
        "     ])\n",
        "\n",
        "decoder_net = tf.keras.Sequential(\n",
        "    [\n",
        "     InputLayer(input_shape=(latent_dim,)),\n",
        "     Dense(7, activation=tf.nn.relu),\n",
        "     Dense(15, activation=tf.nn.relu),\n",
        "     Dense(20, activation=tf.nn.relu),\n",
        "     Dense(n_features, activation=None)\n",
        "     ])\n",
        "    \n",
        "# initialize outlier detector\n",
        "od = OutlierVAE(threshold=None,  # threshold for outlier score\n",
        "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
        "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
        "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
        "                latent_dim=latent_dim,\n",
        "                samples=5)\n",
        "# train\n",
        "od.fit(X_train,\n",
        "       loss_fn=elbo,\n",
        "       cov_elbo=dict(sim=.01),\n",
        "       epochs=30,\n",
        "       verbose=True)\n",
        "\n",
        "# save the trained outlier detector\n",
        "save_detector(od, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGL9gc6UGMXo"
      },
      "source": [
        "The warning tells us we still need to set the outlier threshold. This can be done with the `infer_threshold` method. We need to pass a batch of instances and specify what percentage of those we consider to be normal via `threshold_perc`. Let's assume we have some data which we know contains around 5% outliers. The percentage of outliers can be set with `perc_outlier` in the `create_outlier_batch` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8GbfajFGCZV"
      },
      "source": [
        "np.random.seed(0)\n",
        "perc_outlier = 5\n",
        "threshold_batch = create_outlier_batch(X_train, Y_train, n_samples=100, perc_outlier=perc_outlier)\n",
        "X_threshold, y_threshold = threshold_batch.data.astype('float'), threshold_batch.target\n",
        "X_threshold = (X_threshold - mean) / stdev\n",
        "print('{}% outliers'.format(100 * y_threshold.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWKoCcojGaEI"
      },
      "source": [
        "od.infer_threshold(X_threshold, threshold_perc=100-perc_outlier)\n",
        "print('New threshold: {}'.format(od.threshold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw60AftDGcih"
      },
      "source": [
        "save_detector(od, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMMHiGZXGnQ3"
      },
      "source": [
        "## Detect outliers\n",
        "\n",
        "We now generate a batch of data with 10% outliers and detect the outliers in the batch. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IK5V4PsxeEd"
      },
      "source": [
        "### provide more explanation here of what we're doing and why"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MtuBEz8Ggqp"
      },
      "source": [
        "np.random.seed(1)\n",
        "outlier_batch = create_outlier_batch(X_train, Y_train, n_samples=2886, perc_outlier=10)\n",
        "X_outlier, y_outlier = outlier_batch.data.astype('float'), outlier_batch.target\n",
        "X_outlier = (X_outlier - mean) / stdev\n",
        "print(X_outlier.shape, y_outlier.shape)\n",
        "print('{}% outliers'.format(100 * y_outlier.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQayaNVgGwWn"
      },
      "source": [
        "Predict outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdOja4ZDGtw9"
      },
      "source": [
        "od_preds = od.predict(X_outlier,\n",
        "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
        "                      return_feature_score=True,  # scores used to determine outliers\n",
        "                      return_instance_score=True)\n",
        "print(list(od_preds['data'].keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNDr5nwpG20x"
      },
      "source": [
        "## Display results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOP_-6xvG0Ll"
      },
      "source": [
        "labels = outlier_batch.target_names\n",
        "y_pred = od_preds['data']['is_outlier']\n",
        "f1 = f1_score(y_outlier, y_pred)\n",
        "print('F1 score: {:.4f}'.format(f1))\n",
        "cm = confusion_matrix(y_outlier, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "sns.heatmap(df_cm, annot=True, cbar=True, linewidths=.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv3kkWnZINJl"
      },
      "source": [
        "plot_instance_score(od_preds, y_outlier, labels, od.threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UbNaXO_IShg"
      },
      "source": [
        "roc_data = {'VAE': {'scores': od_preds['data']['instance_score'], 'labels': y_outlier}}\n",
        "plot_roc(roc_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQ9JJGUIpVW"
      },
      "source": [
        "## Investigate instance level outlier\n",
        "\n",
        "We can now take a closer look at some of the individual predictions on `X_outlier`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3FlG8qBIu1A"
      },
      "source": [
        "X_recon = od.vae(X_outlier).numpy()  # reconstructed instances by the VAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7RR5fEkIyK8"
      },
      "source": [
        "plot_feature_outlier_tabular(od_preds,\n",
        "                             X_outlier,\n",
        "                             X_recon=X_recon,\n",
        "                             threshold=od.threshold,\n",
        "                             instance_ids=None,  # pass a list with indices of instances to display\n",
        "                             max_instances=5,  # max nb of instances to display\n",
        "                             top_n=5,  # only show top_n features ordered by outlier score\n",
        "                             outliers_only=False,  # only show outlier predictions\n",
        "                             feature_names=columns,  # add feature names\n",
        "                             figsize=(20, 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efUpXVoKkOZo"
      },
      "source": [
        "### provide a tangiable explained example of an obvious outlier like some large or small mean coupled with high spend in \n",
        "### a single category."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHckSdOuxihv"
      },
      "source": [
        "### Push to GCP and then DEPLOY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkG5fKciIazt"
      },
      "source": [
        "## Drift Detection\n",
        "\n",
        "### Method\n",
        "\n",
        "The drift detector applies feature-wise two-sample [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) (K-S) tests for the continuous numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3FRyfX1KQEO"
      },
      "source": [
        "We split the data in a reference set and 2 test sets on which we test the data drift:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7SCT9OlIZGP"
      },
      "source": [
        "n_ref = 900\n",
        "n_test = 900\n",
        "\n",
        "X_ref, X_t0, X_t1 = X_train[:n_ref], X_train[n_ref:n_ref + n_test], X_train[n_ref + n_test:n_ref + 2 * n_test]\n",
        "X_ref.shape, X_t0.shape, X_t1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anEOqBuOKWWR"
      },
      "source": [
        "### Detect drift\n",
        "\n",
        "Initialize the detector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J63Sj7bSkOZo"
      },
      "source": [
        "# Where are we specifying batch size - does this play a part in the training?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wofaIs0KM3u"
      },
      "source": [
        "cd = TabularDrift(X_ref, p_val=.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQVFSz1-K0Da"
      },
      "source": [
        "preds = cd.predict(X_t0)\n",
        "labels = ['No!', 'Yes!']\n",
        "print('Drift? {}'.format(labels[preds['data']['is_drift']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNeHKA7eK5oL"
      },
      "source": [
        "for f in range(cd.n_features):\n",
        "    stat = 'K-S'\n",
        "    fname = columns[f]\n",
        "    stat_val, p_val = preds['data']['distance'][f], preds['data']['p_val'][f]\n",
        "    print(f'{fname} -- {stat} {stat_val:.3f} -- p-value {p_val:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVHrtyl2LekN"
      },
      "source": [
        "None of the feature-level p-values are below the threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M84FTiNLVCB"
      },
      "source": [
        "preds['data']['threshold']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX2S4WAFLmjr"
      },
      "source": [
        "If you are interested in individual feature-wise drift, this is also possible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_G38h4OLr8X"
      },
      "source": [
        "fpreds = cd.predict(X_t0, drift_type='feature')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5rxryKxLtNx"
      },
      "source": [
        "for f in range(cd.n_features):\n",
        "    stat = 'K-S'\n",
        "    fname = columns[f]\n",
        "    is_drift = fpreds['data']['is_drift'][f]\n",
        "    stat_val, p_val = fpreds['data']['distance'][f], fpreds['data']['p_val'][f]\n",
        "    print(f'{fname} -- Drift? {labels[is_drift]} -- {stat} {stat_val:.3f} -- p-value {p_val:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddrgyHpoL2ZR"
      },
      "source": [
        "preds = cd.predict(X_t1)\n",
        "labels = ['No!', 'Yes!']\n",
        "print('Drift? {}'.format(labels[preds['data']['is_drift']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el-goYR5L8nD"
      },
      "source": [
        "We can again investigate the individual features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS_vpe8XMB2K"
      },
      "source": [
        "for f in range(cd.n_features):\n",
        "    stat = 'K-S'\n",
        "    fname = columns[f]\n",
        "    is_drift = (preds['data']['p_val'][f] < preds['data']['threshold']).astype(int)\n",
        "    stat_val, p_val = preds['data']['distance'][f], preds['data']['p_val'][f]\n",
        "    print(f'{fname} -- Drift? {labels[is_drift]} -- {stat} {stat_val:.3f} -- p-value {p_val:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbYRXNBskOZq"
      },
      "source": [
        "### add a tangiable drift example! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TUfTjQdN2XI"
      },
      "source": [
        "filepath = 'models/DriftDetector'\n",
        "if not os.path.exists(filepath):\n",
        "  os.mkdir(filepath)\n",
        "\n",
        "save_detector(cd, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTze5itgxrjN"
      },
      "source": [
        "### Push to GCP and Deploy \n",
        "!gsutil cp -r models/DriftDetector gs://tom-seldon-examples/datareply-workshop/models/sgreaves/lr/drift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FWFSktgN2XI"
      },
      "source": [
        "DD_URI = 'gs://tom-seldon-examples/datareply-workshop/models/sgreaves/lr/drift'\n",
        "DD_NAME = 'KSDrift-Detector'\n",
        "\n",
        "dd_config = {'deployment': DEPLOYMENT_NAME,\n",
        "             'deployment_namespace': None,\n",
        "             'namespace': 'seldon-logs',\n",
        "             'params': {'drift_batch_size': '10',\n",
        "                        'env_secret_ref': None,\n",
        "                        'event_source': f'io.seldon.serving.dev-seldondeployment-{DEPLOYMENT_NAME}-drift',\n",
        "                        'event_type': 'io.seldon.serving.inference.drift',\n",
        "                        'http_port': '8080',\n",
        "                        'model_name': DD_NAME,\n",
        "                        'protocol': 'seldon.http',\n",
        "                        'reply_url': 'http://seldon-request-logger.seldon-logs',\n",
        "                        'storage_uri': DD_URI,\n",
        "                        'user_permission': None},\n",
        "             'prom_scraping': None,\n",
        "             'url': None}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36jqHmIzN2XI"
      },
      "source": [
        "dd_api = DriftDetectorApi(auth())\n",
        "dd_api.create_drift_detector_seldon_deployment(name=DEPLOYMENT_NAME,\n",
        "                                               namespace=\"dev\",\n",
        "                                               drift_detector=dd_config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}